{"componentChunkName":"component---src-templates-post-js","path":"/posts/sentence-banking","result":{"data":{"markdownRemark":{"id":"764daddb-2e4f-5ba6-b7ff-2ac26f75ce10","html":"<p>This is a post about <a href=\"https://github.com/cronokirby/ginkou\">ginkou</a>, a tool I\nmade recently. This tool uses Rust, SQLite, as well as\n<a href=\"http://taku910.github.io/mecab/\">mecab</a> to archive sentences, and then to\nretrieve them based on the words they contain.</p>\n<h1>Why would I need this?</h1>\n<p>Unless you're learning Japanese, you probably won't.</p>\n<p>With the way I'm approaching it, I try and combine words and grammar into the\nsame flashcard system. This means making new flashcarsd with sentences\ncontaining small bits of information I'm trying to learn. These can be new words\nof grammar. By keeping information in full sentences, everything is learned in context.</p>\n<p>A new words is sometimes encountered without an accompanying sentence, or in a\nsentence that contain too many unknown parts. We want a sentence where just that\nword is new, so we'd like to easily find sentences containing that word.</p>\n<p><strong>Ginkou</strong> makes finding these examples sentences easy. The program is used to\nfirst build up a bank of sentences, and then to retrieve sentences from that\nbank based on the words they contain.</p>\n<h1>Bird's eye view</h1>\n<p><strong>Ginkou</strong> has two main operations: <strong>add</strong> and <strong>get</strong>.</p>\n<p>The <strong>add</strong> operation takes text, either from a file, or from the command line,\nand then adds those to the bank. This operation will first parse the input into\nsentences. Then it splits each sentence into a list of words, using <strong>mecab</strong>.\nUsing <strong>mecab</strong> instead of naive splitting lets us use the root from for\nconjugated verbs. Naively splitting would use the form the verb happens to be\nconjugated in.</p>\n<p>We then store the sentence in one table, and each word in that sentence in\nanother. We then add a link in a junction table for each word. Each link\nspecifies that a word <code class=\"language-text\">W</code> appears in a sentence <code class=\"language-text\">S</code>. This table is used to\nrepresent the <em>many-to-many</em> relationship between words and sentences.</p>\n<p>The <strong>get</strong> operation looks up the sentences containing a given word. This\noperation uses the junction table built up with the <strong>add</strong> operation.\nImplementing this operation requires a bit of <em>SQL</em> statement savvy, which we'll explore later in\nthis post.</p>\n<h1>Why Rust?</h1>\n<p>I like using <em>Rust</em> for small command line tools. It has a bit of a learning\ncurve, but is easy to use after that. The tools tend to work efficiently without\nhaving to pine much over performance. <em>Rust</em> also has good libraries for parsing\ncommand line arguments, and for using <em>SQLite</em>.</p>\n<h1>Why SQLite?</h1>\n<p>One advantage of using SQLite was the ability to transfer the bank between\ncomputers easily. Since SQLite keeps a database in a single file,\nwe can simply transfer the file from one place to another.</p>\n<p>Integrating SQLite is also much easier in a standalone application,\nas we don’t need to worry about starting the database in the background.\nAll we need to do is have a file for SQLite to work with.</p>\n<h1>Table Structure</h1>\n<p>In this section we'll go over what our database schema looks like.</p>\n<p>We have a table for each word, and a table for each sentence:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> Words<span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">INTEGER</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">,</span>\n    word <span class=\"token keyword\">TEXT</span> <span class=\"token keyword\">UNIQUE</span> <span class=\"token operator\">NOT</span> <span class=\"token boolean\">NULL</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> Sentences<span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">INTEGER</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">,</span>\n    sentence <span class=\"token keyword\">TEXT</span> <span class=\"token operator\">NOT</span> <span class=\"token boolean\">NULL</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Each table uses an integer as its primary key. Both tables then store text in\nthe other column. We don't want to avoid storing duplicate words, so we add a\n<code class=\"language-text\">UNIQUE</code> constraint to that column. To avoid errors, we need to make sure to\nonly insert new words into the table.</p>\n<p>Next we have the junction table modelling the <em>many-to-many</em> relationship:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> WordSentence<span class=\"token punctuation\">(</span>\n    word_id <span class=\"token keyword\">INTEGER</span> <span class=\"token operator\">NOT</span> <span class=\"token boolean\">NULL</span><span class=\"token punctuation\">,</span>\n    sentence_id <span class=\"token keyword\">INTEGER</span> <span class=\"token operator\">NOT</span> <span class=\"token boolean\">NULL</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">(</span>word_id<span class=\"token punctuation\">,</span> sentence_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">FOREIGN</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">(</span>word_id<span class=\"token punctuation\">)</span> <span class=\"token keyword\">REFERENCES</span> Words<span class=\"token punctuation\">(</span>id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">FOREIGN</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">(</span>sentence_id<span class=\"token punctuation\">)</span> <span class=\"token keyword\">REFERENCES</span> Sentences<span class=\"token punctuation\">(</span>id<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>This table only contains 2 columns, one referencing words, and the other\nreferencing sentences. The rest of the schema makes sure that the keys inserted\ninto the table come in unique pairs, and that they reference keys that exist in\nthe other tables.</p>\n<h2>Example usage</h2>\n<p>To illustrate how this table works, let's look at an example sentence. The\nsentence \"猫を見た\" contains the following words: 猫, を, and 見る. (見た is the\npast tense of 見る). Adding this sentence to an empty database will give us the\nfollowing tables:</p>\n<p><strong>Words</strong>:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">id word\n<span class=\"token comment\">-- ----</span>\n<span class=\"token number\">1</span>  猫\n<span class=\"token number\">2</span>  を\n<span class=\"token number\">3</span>  見る</code></pre></div>\n<p><strong>Sentences</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">id sentence\n<span class=\"token comment\">-- --------</span>\n<span class=\"token number\">1</span>  猫を見た</code></pre></div>\n<p><strong>WordSentence</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">word_id sentence_id\n<span class=\"token comment\">------- -----------</span>\n<span class=\"token number\">1</span>       <span class=\"token number\">1</span>\n<span class=\"token number\">2</span>       <span class=\"token number\">1</span>\n<span class=\"token number\">3</span>       <span class=\"token number\">1</span></code></pre></div>\n<p>For each word, we've created a link in the junction table representing the word\nbelonging to the first sentence in our newly populated sentence table.</p>\n<h2>Big Scary Statements</h2>\n<p>The trickiest statement to write was for the <code class=\"language-text\">get</code> operation.\nThis statement needs to look up all the sentences containing a specific word.\nTo do this we make use of the Junction table we previously populated,\njoining our sentences with the words they contain.\nOnce we have a table of rows with a sentence and a word,\nwe can filter for rows containing the right word, and take out the sentence.</p>\n<p>The statement looks like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> sentence <span class=\"token keyword\">FROM</span> sentences\n<span class=\"token keyword\">LEFT</span> <span class=\"token keyword\">JOIN</span> wordsentence <span class=\"token keyword\">ON</span> wordsentence<span class=\"token punctuation\">.</span>sentence_id <span class=\"token operator\">=</span> sentences<span class=\"token punctuation\">.</span>id\n<span class=\"token keyword\">LEFT</span> <span class=\"token keyword\">JOIN</span> words <span class=\"token keyword\">ON</span> words<span class=\"token punctuation\">.</span>id <span class=\"token operator\">=</span> wordsentence<span class=\"token punctuation\">.</span>word_id\n<span class=\"token keyword\">WHERE</span> word<span class=\"token operator\">=</span>?<span class=\"token number\">1</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>I think we could have used an <em>Inner Join</em> as well,\nbut since we’re checking with equality on a word,\nthe <code class=\"language-text\">NULL</code>s that appear because of a <em>Left Join</em> don’t matter.</p>\n<h1>Disks are slow</h1>\n<p>On the first iteration of the program, consuming sentences was very slow:\nthe program was only capable of adding 3 sentences per second or so.\nThe culprit turned out to be how SQLite was used.</p>\n<p>When SQLite was used with an in memory database instead of an on-disk one,\nthe processing rate went up to 1000 sentences per second.\nEvery time we added a sentence, we had to process a few SQL statements on the\ndatabase. With an on file database, this meant hitting the disk for every sentence, which was quite slow.</p>\n<p>In order to take advantage of the speed of in memory transactions,\nwhile still having a final database on disk, I used SQLite’s transactions,\nwhich allow us to perform a bunch of operations in memory,\nbefore finally committing them to the real database on disk.</p>\n<h1>Final Remarks</h1>\n<p>This post was just to share a few thoughts and snippets of what went into this\nlittle project. Hopefully there was something to learn from it.\nThe curious can check out the code over <a href=\"https://github.com/cronokirby/ginkou\">here</a>.</p>","frontmatter":{"date":"July 07, 2019","image":"/print9.jpg","path":"/posts/sentence-banking","title":"Sentence Banking","description":"How I used Rust, SQLite and mecab to organize Japanese sentences"}}},"pageContext":{}}}